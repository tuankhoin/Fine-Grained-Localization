{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIFT Matches Extraction\n",
    "\n",
    "This script exports for each test image its number of SIFT matches with other training images, with the corresponding image filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tqdm\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_i = []\n",
    "test_i = []\n",
    "\n",
    "train_xy = pd.read_csv('validate_train.csv')\n",
    "train_path = train_xy['id'].values\n",
    "test_xy =  pd.read_csv('validate.csv')\n",
    "test_path = test_xy['id'].values\n",
    "\n",
    "for f in train_path:\n",
    "    i = cv2.imread('./train/' + f + '.jpg')\n",
    "    train_i.append(i)\n",
    "for f in test_path:\n",
    "    i = cv2.imread('./train/' + f + '.jpg')\n",
    "    test_i.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to SIFT coordinates\n",
    "Since the dataset can be heavy (up to 5GB), the dataset is exported to pickle file instead, so that:\n",
    "* The data can be stored in internal disk rather than RAM\n",
    "* The process can start from anywhere you want. No need to worry if there is anything broken halfway "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2035it [03:32,  9.56it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8844/2458770662.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Compute SIFT keypoints and descriptors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msift\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'./train_kp/train_kp{train_path[i]}.pckl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Compute SIFT keypoints and descriptors\n",
    "for i,img in tqdm.tqdm(enumerate(train_i)):\n",
    "    _, des = sift.detectAndCompute(img,None)\n",
    "    f = open(f'./train_kp/train_kp{train_path[i]}.pckl','wb')\n",
    "    pickle.dump(des,f)\n",
    "    f.close()\n",
    "\n",
    "for i,img in tqdm.tqdm(enumerate(test_i)):\n",
    "    _, des = sift.detectAndCompute(img,None)\n",
    "    f = open(f'./test_kp/test_kp{test_path[i]}.pckl','wb')\n",
    "    pickle.dump(des,f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the extraction\n",
    "\n",
    "* Read in the previously exported data\n",
    "* Just letting it run. Each batch of 100 images takes around 8 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/600 [00:17<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8844/2337939415.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mgoods\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflann\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mknnMatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdes_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdes_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[1;31m# Store all good matches based on Lowe's Ratio test.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mgood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmatches\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# FLANN matcher\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "for test in tqdm.tqdm(test_path):\n",
    "    # Read in file\n",
    "    f = open(f'./train_kp/train_kp{test}.pckl', 'rb')\n",
    "    des_test = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    goods = []\n",
    "    current_good = 0\n",
    "    # Not matchable: Export None to a dummy pickle file\n",
    "    if des_test is None or len(des_test) < 2:\n",
    "        with open(f'./not_match/sift_not_match_{test}.pckl','rb') as f:\n",
    "            pickle.dump(None,f)\n",
    "    else:\n",
    "        for train in train_path:\n",
    "            f = open(f'./train_kp/train_kp{train}.pckl', 'rb')\n",
    "            des_train = pickle.load(f)\n",
    "            f.close()\n",
    "\n",
    "            # If matching is weak, it must be irrelevant. Assign -1 to no. matches\n",
    "            if des_train is None or len(des_train) < 2:\n",
    "                goods.append(-1)\n",
    "                continue\n",
    "            matches = flann.knnMatch(des_test,des_train,k=2)\n",
    "            # Store all good matches based on Lowe's Ratio test.\n",
    "            good = len([m for m,n in matches if m.distance < 0.7*n.distance])\n",
    "            goods.append(good)\n",
    "        \n",
    "        # Sorting. The order starts from the least matches\n",
    "        sorted_idx = np.argsort(goods)\n",
    "        sorted_path = [train_path[idx] for idx in sorted_idx]\n",
    "        sorted_goods = [goods[idx] for idx in sorted_idx]\n",
    "        \n",
    "    # Export to pickle again\n",
    "    f = open(f'./filename/sift_name_{test}.pckl','wb')\n",
    "    pickle.dump(sorted_path,f)\n",
    "    f.close()\n",
    "    f = open(f'./goodmatch/sift_good_n_{test}.pckl','wb')\n",
    "    pickle.dump(sorted_goods,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the matched files to completed `.csv` files\n",
    "Also flip back the order to be more consistent with CNN feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [04:37<00:00,  4.32it/s]\n"
     ]
    }
   ],
   "source": [
    "n_trains = len(train_xy)\n",
    "# Get list of weak test instances\n",
    "weak_test = [path.replace('not_match\\\\sift_not_match_','').replace('.pckl','') \n",
    "             for path in glob.glob('not_match/*')]\n",
    "\n",
    "fname = np.empty((0,n_trains))\n",
    "match = np.empty((0,n_trains))\n",
    "for test in tqdm(test_path):\n",
    "    if test in weak_test:\n",
    "        fname = np.append(fname, np.empty((1,n_trains)) * (np.nan), axis=0)\n",
    "        match = np.append(match, np.empty((1,n_trains)) * (np.nan), axis=0)\n",
    "        continue\n",
    "    with open(f'./filename/sift_name_{test}.pckl','rb') as f:\n",
    "        fn = np.flip(pickle.load(f))\n",
    "    with open(f'./goodmatch/sift_good_n_{test}.pckl','rb') as f:\n",
    "        goods = np.flip(pickle.load(f))\n",
    "    # Assign nan row for instances with weak matches\n",
    "    if fn is None:\n",
    "        fname = np.append(fname, np.empty((1,n_trains)) * (np.nan), axis=0)\n",
    "        match = np.append(match, np.empty((1,n_trains)) * (np.nan), axis=0)\n",
    "        continue\n",
    "    fname = np.append(fname, [fn], axis=0)\n",
    "    match = np.append(match, [goods], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(fname).to_csv('sift_matches_filename.csv',index=False)\n",
    "pd.DataFrame(match).to_csv('sift_matches_distance.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4621005da5c26ac209901ca167bf25025457b064ec855aea9ba97365ac8d4984"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
