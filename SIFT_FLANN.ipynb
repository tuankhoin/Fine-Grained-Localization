{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIFT Matches Extraction\n",
    "\n",
    "This script exports for each test image its number of SIFT matches with other training images, with the corresponding image filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tqdm\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_i = []\n",
    "test_i = []\n",
    "\n",
    "train_xy = pd.read_csv('train.csv')\n",
    "train_path = train_xy['id'].values\n",
    "test_path = pd.read_csv('imagenames.csv')['id'].values\n",
    "\n",
    "for f in train_path:\n",
    "    i = cv2.imread('./train/' + f + '.jpg')\n",
    "    train_i.append(i)\n",
    "for f in test_path:\n",
    "    i = cv2.imread('./test/' + f + '.jpg')\n",
    "    test_i.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to SIFT coordinates\n",
    "Since the dataset can be heavy (up to 5GB), the dataset is exported to pickle file instead, so that:\n",
    "* The data can be stored in internal disk rather than RAM\n",
    "* The process can start from anywhere you want. No need to worry if there is anything broken halfway "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7500it [11:35, 10.78it/s]\n",
      "1200it [01:56, 10.31it/s]\n"
     ]
    }
   ],
   "source": [
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Compute SIFT keypoints and descriptors\n",
    "for i,img in tqdm.tqdm(enumerate(train_i)):\n",
    "    _, des = sift.detectAndCompute(img,None)\n",
    "    f = open(f'./train_kp/train_kp{train_path[i]}.pckl','wb')\n",
    "    pickle.dump(des,f)\n",
    "    f.close()\n",
    "\n",
    "for i,img in tqdm.tqdm(enumerate(test_i)):\n",
    "    _, des = sift.detectAndCompute(img,None)\n",
    "    f = open(f'./test_kp/test_kp{test_path[i]}.pckl','wb')\n",
    "    pickle.dump(des,f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the extraction\n",
    "\n",
    "* Read in the previously exported data\n",
    "* Just letting it run. Each batch of 100 images takes around 8 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLANN matcher\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "for test in tqdm.tqdm(test_path):\n",
    "    # Read in file\n",
    "    f = open(f'./test_kp/test_kp{test}.pckl', 'rb')\n",
    "    des_test = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    goods = []\n",
    "    current_good = 0\n",
    "    # Not matchable: Export None to a dummy pickle file\n",
    "    if des_test is None or len(des_test) < 2:\n",
    "        with open(f'./not_match/sift_not_match_{test}.pckl','rb') as f:\n",
    "            pickle.dump(None,f)\n",
    "    else:\n",
    "        for train in train_path:\n",
    "            f = open(f'./train_kp/train_kp{train}.pckl', 'rb')\n",
    "            des_train = pickle.load(f)\n",
    "            f.close()\n",
    "\n",
    "            # If matching is weak, it must be irrelevant. Assign -1 to no. matches\n",
    "            if des_train is None or len(des_train) < 2:\n",
    "                goods.append(-1)\n",
    "                continue\n",
    "            matches = flann.knnMatch(des_test,des_train,k=2)\n",
    "            # Store all good matches based on Lowe's Ratio test.\n",
    "            good = len([m for m,n in matches if m.distance < 0.7*n.distance])\n",
    "            goods.append(good)\n",
    "        \n",
    "        # Sorting. The order starts from the least matches\n",
    "        sorted_idx = np.argsort(goods)\n",
    "        sorted_path = [train_path[idx] for idx in sorted_idx]\n",
    "        sorted_goods = [goods[idx] for idx in sorted_idx]\n",
    "        \n",
    "    # Export to pickle again\n",
    "    f = open(f'./filename/sift_name_{test}.pckl','wb')\n",
    "    pickle.dump(sorted_path,f)\n",
    "    f.close()\n",
    "    f = open(f'./goodmatch/sift_good_n_{test}.pckl','wb')\n",
    "    pickle.dump(sorted_goods,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the matched files to completed `.csv` files\n",
    "Also flip back the order to be more consistent with CNN feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [04:37<00:00,  4.32it/s]\n"
     ]
    }
   ],
   "source": [
    "n_trains = len(train_xy)\n",
    "# Get list of weak test instances\n",
    "weak_test = [path.replace('not_match\\\\sift_not_match_','').replace('.pckl','') \n",
    "             for path in glob.glob('not_match/*')]\n",
    "\n",
    "fname = np.empty((0,n_trains))\n",
    "match = np.empty((0,n_trains))\n",
    "for test in tqdm(test_path):\n",
    "    if test in weak_test:\n",
    "        fname = np.append(fname, np.empty((1,n_trains)) * (np.nan), axis=0)\n",
    "        match = np.append(match, np.empty((1,n_trains)) * (np.nan), axis=0)\n",
    "        continue\n",
    "    with open(f'./filename/sift_name_{test}.pckl','rb') as f:\n",
    "        fn = np.flip(pickle.load(f))\n",
    "    with open(f'./goodmatch/sift_good_n_{test}.pckl','rb') as f:\n",
    "        goods = np.flip(pickle.load(f))\n",
    "    # Assign nan row for instances with weak matches\n",
    "    if fn is None:\n",
    "        fname = np.append(fname, np.empty((1,n_trains)) * (np.nan), axis=0)\n",
    "        match = np.append(match, np.empty((1,n_trains)) * (np.nan), axis=0)\n",
    "        continue\n",
    "    fname = np.append(fname, [fn], axis=0)\n",
    "    match = np.append(match, [goods], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(fname).to_csv('sift_matches_filename.csv',index=False)\n",
    "pd.DataFrame(match).to_csv('sift_matches_distance.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4621005da5c26ac209901ca167bf25025457b064ec855aea9ba97365ac8d4984"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
